"""
Simple distributions of short-sequence discrete data.

Parts of this code were modified from
https://github.com/facebookresearch/semi-discrete-flow
which is under Attribution-NonCommercial 4.0 International license.
The full license text can be found at
https://github.com/facebookresearch/semi-discrete-flow/blob/main/LICENSE

Parts of this code were further modified from
https://github.com/HannesStark/dirichlet-flow-matching
which is under MIT License.
The full license text can be found at
https://github.com/HannesStark/dirichlet-flow-matching/blob/main/LICENSE
"""
import numpy as np
import torch
from torch.nn.functional import one_hot
import os.path
from torch.distributions.dirichlet import Dirichlet
from torch.special import entr

class SampleGenerator(object):
    """Iterable generating sample batches from a simple data distribution."""
    def __init__(self, data_distr, num_batches, batch_size=1, **unused_kwargs):
        super(SampleGenerator, self).__init__()
        self.data_distr = data_distr
        self.num_batches = num_batches
        self.batch_size = batch_size
        self.current_batch = 0
    
    def __iter__(self):
        self.current_batch = 0
        return self

    def __len__(self) -> int:
        return self.num_batches

    def __next__(self) -> torch.Tensor:
        if self.current_batch == self.num_batches:
            raise StopIteration
        self.current_batch += 1
        w = self.data_distr.sample_smoothed_assignment(self.batch_size)
        return w

class SimpleDataDistr(object):
    """Simple, n=2 data distributions with explicitly available joint probability mass function"""
    def __init__(self, dataset_params, joint_pmf):
        super(SimpleDataDistr, self).__init__()
        self.joint_pmf = joint_pmf
        self.c = self.joint_pmf.shape[0]
        self.n = self.joint_pmf.ndim
        self.batches_per_epoch = dataset_params["batches_per_epoch"]
        self.smoothing = dataset_params["integer_smoothing"]
    
    def sample(self, batch_size) -> torch.Tensor:
        """
        Draw a sample from this distribution.
        The output is a long tensor (labeling) with shape (batch_size, n), n=2
        """
        if isinstance(batch_size, int):
            batch_size = (batch_size,)
        probs = self.joint_pmf.reshape(-1)
        probs = torch.from_numpy(probs)
        sample = torch.multinomial(probs, int(np.prod(batch_size)), replacement=True)
        if self.n == 1:
            return sample.unsqueeze(-1)
        x = torch.div(sample, self.c, rounding_mode="floor")
        y = sample % self.c
        return torch.stack([x, y], axis=-1).reshape(*batch_size, 2)

    def sample_assignment(self, batch_size) -> torch.Tensor:
        """
        Same as self.sample, but returns an integer assignment matrix 
        with shape (batch_size, c, n), c=n=2
        """
        s = self.sample(batch_size)
        w = torch.swapaxes(one_hot(s, num_classes=self.c), 1, 2)
        return w

    def sample_smoothed_assignment(self, batch_size) -> torch.Tensor:
        """
        Draw sample w on assignment manifold coner and return 
        smoothing*barycenter + (1-smoothing)*w
        output has shape (batch_size, c, n), n=2
        """
        w = self.sample_assignment(batch_size)
        bc = torch.ones_like(w)/self.c
        return self.smoothing*bc + (1-self.smoothing)*w

    def mse_from_hist(self, p) -> torch.Tensor:
        """
        Mean squared error of exact joint probability mass function
        to the normalized histogram p generated from samples.
        """
        return torch.square(p.flatten() - torch.from_numpy(self.joint_pmf).flatten()).mean()

    def kl_from_hist(self, p) -> torch.Tensor:
        """
        Relative entropy of exact joint probability mass function
        to the normalized histogram p generated from samples.
        """
        pmf = torch.from_numpy(self.joint_pmf).flatten()
        pmf = torch.clamp(pmf, min=1e-7) # this is a hack countering numerical underflow
        hist = p.flatten()
        return -entr(hist).sum() - (hist*torch.log(pmf)).sum()

    def hist_from_samples(self, labelings) -> torch.Tensor:
        """
        Empirical PMF as 2D array generated by normalizing the
        histogram of integer samples `labelings`.
        """
        if self.n == 1:
            bins = torch.arange(self.c+1).float()-0.5
            assert labelings.shape[-1] == 1
            hist = torch.histogram(labelings.squeeze(-1).float(), bins=bins, density=True).hist
            return hist.reshape(self.c)
        assert self.n == 2
        N = self.c**self.n
        flattened_labeling = (labelings[:,0]*self.c + labelings[:,1]).float()
        bins = torch.arange(N+1).float()-0.5
        flat_hist = torch.histogram(flattened_labeling, bins=bins, density=True).hist
        return flat_hist.reshape(self.c, self.c)

    def dataloader(self, split: str = "train", **kwargs) -> SampleGenerator:
        return SampleGenerator(self, self.batches_per_epoch, **kwargs)

class CoupledBinaryDistribution(SimpleDataDistr):
    """Two almost deterministically coupled binary variables."""
    def __init__(self, dataset_params):
        self.eps = dataset_params["eps"]
        self.c = 2
        joint_pmf = np.array([[0.5-self.eps, self.eps], [self.eps, 0.5-self.eps]])
        super(CoupledBinaryDistribution, self).__init__(dataset_params, joint_pmf)

    def tensor_format(self):
        """
        Loaded data will have this tensor shape. 
        Batch dimension(s) are indicated by -1.
        """
        return (-1, self.c, 2)

    def mse_empirical(self, W):
        """
        Given an assignment matrix W of shape (batch_size, c, n),
        count empirical frequencies of each class combination,
        compare them to the ground truth probabilities 
        and return the mean squared error
        The batch size should be somewhat large (about 1000) to get
        reliable empirical estimates (+= 1e-3 mse)
        """
        if not isinstance(self, CoupledBinary):
            print("Empirical MSE is only implemented for CoupledBinary distribution.")
            raise NotImplementedError
        labelings = W.argmax(dim=1)
        bin1 = torch.logical_and(labelings[:,0] == 0, labelings[:,1] == 0).float().mean() # (0,0)
        bin2 = torch.logical_and(labelings[:,0] == 0, labelings[:,1] == 1).float().mean() # (0,1)
        bin3 = torch.logical_and(labelings[:,0] == 1, labelings[:,1] == 0).float().mean() # (1,0)
        bin4 = torch.logical_and(labelings[:,0] == 1, labelings[:,1] == 1).float().mean() # (1,1)
        empirical = torch.tensor([bin1, bin2, bin3, bin4])
        return torch.square(empirical - torch.from_numpy(self.joint_pmf).flatten()).mean()

class PinwheelDistribution(SimpleDataDistr):
    """
    Modified from https://github.com/facebookresearch/semi-discrete-flow
    """
    def __init__(self, dataset_params):
        data_dir = "data/simple_distr"
        file_path = os.path.join(data_dir, "discrete_pinwheel_pmf.csv")
        joint_pmf = np.loadtxt(file_path, delimiter=",")
        super(PinwheelDistribution, self).__init__(dataset_params, joint_pmf)

    def tensor_format(self):
        """
        Loaded data will have this tensor shape. 
        Batch dimension(s) are indicated by -1.
        """
        return (-1, self.c, 2)

class GaussianMixtureDistribution(SimpleDataDistr):
    """
    Modified from https://github.com/facebookresearch/semi-discrete-flow
    """
    def __init__(self, dataset_params):
        data_dir = "data/simple_distr"
        file_path = os.path.join(data_dir, "discrete_8gaussians_pmf.csv")
        joint_pmf = np.loadtxt(file_path, delimiter=",")
        super(GaussianMixtureDistribution, self).__init__(dataset_params, joint_pmf)

    def tensor_format(self):
        """
        Loaded data will have this tensor shape. 
        Batch dimension(s) are indicated by -1.
        """
        return (-1, self.c, 2)

class SingleSimplexDistribution(SimpleDataDistr):
    """Variables simplex dimension distribution of Dirichlet Flow-matching paper"""
    def __init__(self, dataset_params):
        torch.manual_seed(0)
        d = Dirichlet(torch.ones(dataset_params["num_classes"]))
        joint_pmf = d.sample().numpy()
        joint_pmf[0] = 1e-5
        joint_pmf /= joint_pmf.sum()
        super(SingleSimplexDistribution, self).__init__(dataset_params, joint_pmf)
    
    def tensor_format(self):
        """
        Loaded data will have this tensor shape. 
        Batch dimension(s) are indicated by -1.
        """
        return (-1, self.c, 1)

class StarkSimplexDistribution(object):
    """
    Modified from https://github.com/HannesStark/dirichlet-flow-matching
    """
    def __init__(self, dataset_params):
        super().__init__()
        self.num_cls = 1
        self.n = 4
        self.c = dataset_params["num_classes"]
        self.batches_per_epoch = dataset_params["batches_per_epoch"]
        self.smoothing = dataset_params["integer_smoothing"]

        ckpt_path = os.path.join("data/simple_distr", f"stark_simplex_{self.c}.pt")
        if os.path.isfile(ckpt_path):
            distribution_dict = torch.load(ckpt_path)
            self.probs = distribution_dict['probs']
            self.class_probs = distribution_dict['class_probs']
        else:
            self.probs = torch.softmax(torch.rand((self.num_cls, self.n, self.c)), dim=2)
            self.class_probs = torch.ones(self.num_cls)
            if self.num_cls > 1:
                self.class_probs = self.class_probs * 1 / 2 / (self.num_cls - 1)
                self.class_probs[0] = 1 / 2
            assert self.class_probs.sum() == 1

            distribution_dict = {'probs': self.probs, 'class_probs': self.class_probs}
            torch.save(distribution_dict, ckpt_path)

    def sample(self, batch_size):
        batch = torch.empty(batch_size, 4, dtype=torch.long)
        for j in range(self.n):
            batch[:,j] = torch.multinomial(input=self.probs[0,j,:], replacement=True, num_samples=batch_size)
        return batch

    def sample_assignment(self, batch_size):
        """
        Same as self.sample, but returns an integer assignment matrix 
        with shape (batch_size, n, c)
        """
        s = self.sample(batch_size)
        w = one_hot(s, num_classes=self.c)
        return w

    def sample_smoothed_assignment(self, batch_size):
        """
        Draw sample w on assignment manifold coner and return 
        smoothing*barycenter + (1-smoothing)*w
        output has shape (batch_size, c, n), n=2
        """
        w = self.sample_assignment(batch_size)
        bc = torch.ones_like(w)/self.c
        return self.smoothing*bc + (1-self.smoothing)*w

    def hist_from_samples(self, labelings: torch.Tensor) -> torch.Tensor:
        """
        Empirical PMF as 2D array generated by normalizing the
        histogram of integer samples `labelings`.
        """
        hist = torch.zeros(self.n, self.c)
        bins = torch.arange(self.c+1).float()-0.5
        assert labelings.shape[-1] == self.n
        for i in range(self.n):
            hist[i,:] = torch.histogram(labelings[:,i].float(), bins=bins, density=True).hist
        return hist

    def kl_from_hist(self, p):
        kl = -entr(p).sum(-1).mean() - (p*torch.log(self.probs[0])).sum(-1).mean()
        return kl

    def tensor_format(self):
        """
        Loaded data will have this tensor shape. 
        Batch dimension(s) are indicated by -1.
        """
        return (-1, 4, self.c)

    def dataloader(self, **kwargs):
        return SampleGenerator(self, self.batches_per_epoch, **kwargs)